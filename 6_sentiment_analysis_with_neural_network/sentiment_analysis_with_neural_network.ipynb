{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Stock Sentiment from Twits\n",
    "\n",
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "When deciding the value of a company, it's important to follow the news. For example, a product recall or natural disaster in a company's product chain. You want to be able to turn this information into a signal. Currently, the best tool for the job is a Neural Network. \n",
    "\n",
    "I'll use posts from the social media site [StockTwits](https://en.wikipedia.org/wiki/StockTwits). The community on StockTwits is full of investors, traders, and entrepreneurs. Each message posted is called a Twit. This is similar to Twitter's version of a post, called a Tweet. I'll build a model around these twits that generate a sentiment score.\n",
    "\n",
    "To capture the degree of sentiment, a five-point scale was used: very negative, negative, neutral, positive, very positive. Each twit is labeled -2 to 2 in steps of 1, from very negative to very positive respectively. I'll build a sentiment analysis model that will learn to assign sentiment to twits on its own, using this labeled data.\n",
    "\n",
    "\n",
    "\n",
    "## Import Twits \n",
    "### Load Twits Data \n",
    "This JSON file contains a list of objects for each twit in the `'data'` field:\n",
    "\n",
    "```\n",
    "{'data':\n",
    "  {'message_body': 'Neutral twit body text here',\n",
    "   'sentiment': 0},\n",
    "  {'message_body': 'Happy twit body text here',\n",
    "   'sentiment': 1},\n",
    "   ...\n",
    "}\n",
    "```\n",
    "\n",
    "The fields represent the following:\n",
    "\n",
    "* `'message_body'`: The text of the twit.\n",
    "* `'sentiment'`: Sentiment score for the twit, ranges from -2 to 2 in steps of 1, with 0 being neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'message_body': '$FITB great buy at 26.00...ill wait', 'sentiment': 2, 'timestamp': '2018-07-01T00:00:09Z'}, {'message_body': '@StockTwits $MSFT', 'sentiment': 1, 'timestamp': '2018-07-01T00:00:42Z'}, {'message_body': '#STAAnalystAlert for $TDG : Jefferies Maintains with a rating of Hold setting target price at USD 350.00. Our own verdict is Buy  http://www.stocktargetadvisor.com/toprating', 'sentiment': 2, 'timestamp': '2018-07-01T00:01:24Z'}, {'message_body': '$AMD I heard there’s a guy who knows someone who thinks somebody knows something - on StockTwits.', 'sentiment': 1, 'timestamp': '2018-07-01T00:01:47Z'}, {'message_body': '$AMD reveal yourself!', 'sentiment': 0, 'timestamp': '2018-07-01T00:02:13Z'}, {'message_body': '$AAPL Why the drop? I warren Buffet taking out his position?', 'sentiment': 1, 'timestamp': '2018-07-01T00:03:10Z'}, {'message_body': '$BA bears have 1 reason on 06-29 to pay more attention https://dividendbot.com?s=BA', 'sentiment': -2, 'timestamp': '2018-07-01T00:04:09Z'}, {'message_body': '$BAC ok good we&#39;re not dropping in price over the weekend, lol', 'sentiment': 1, 'timestamp': '2018-07-01T00:04:17Z'}, {'message_body': '$AMAT - Daily Chart, we need to get back to above 50.', 'sentiment': 2, 'timestamp': '2018-07-01T00:08:01Z'}, {'message_body': '$GME 3% drop per week after spike... if no news in 3 months, back to 12s... if BO, then bingo... what is the odds?', 'sentiment': -2, 'timestamp': '2018-07-01T00:09:03Z'}]\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join('..', '..', 'data', 'project_6_stocktwits', 'twits.json'), 'r') as f:\n",
    "    twits = json.load(f)\n",
    "\n",
    "print(twits['data'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1548010\n"
     ]
    }
   ],
   "source": [
    "\"\"\"print out the number of twits\"\"\"\n",
    "\n",
    "print(len(twits['data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Message Body and Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [twit['message_body'] for twit in twits['data']]\n",
    "\n",
    "# Since the sentiment scores are discrete, we'll scale the sentiments to 0 to 4 for use in our network\n",
    "sentiments = [twit['sentiment'] + 2 for twit in twits['data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$FITB great buy at 26.00...ill wait',\n",
       " '@StockTwits $MSFT',\n",
       " '#STAAnalystAlert for $TDG : Jefferies Maintains with a rating of Hold setting target price at USD 350.00. Our own verdict is Buy  http://www.stocktargetadvisor.com/toprating',\n",
       " '$AMD I heard there’s a guy who knows someone who thinks somebody knows something - on StockTwits.',\n",
       " '$AMD reveal yourself!',\n",
       " '$AAPL Why the drop? I warren Buffet taking out his position?',\n",
       " '$BA bears have 1 reason on 06-29 to pay more attention https://dividendbot.com?s=BA',\n",
       " '$BAC ok good we&#39;re not dropping in price over the weekend, lol',\n",
       " '$AMAT - Daily Chart, we need to get back to above 50.',\n",
       " '$GME 3% drop per week after spike... if no news in 3 months, back to 12s... if BO, then bingo... what is the odds?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data\n",
    "With the data in hand I will preprocess our text. These twits are collected by filtering on ticker symbols where these are denoted with a leader $ symbol in the twit itself. For example,\n",
    "\n",
    "`{'message_body': 'RT @google Our annual look at the year in Google blogging (and beyond) http://t.co/sptHOAh8 $GOOG',\n",
    " 'sentiment': 0}`\n",
    "\n",
    "The ticker symbols don't provide information on the sentiment, and they are in every twit, so I will remove them. This twit also has the `@google` username, again not providing sentiment information, so I should also remove it. I also see a URL `http://t.co/sptHOAh8`. I'll remove these too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "\n",
    "def preprocess(message):\n",
    "    \"\"\"\n",
    "    This function takes a string as input, then performs these operations: \n",
    "        - lowercase\n",
    "        - remove URLs\n",
    "        - remove ticker symbols \n",
    "        - removes punctuation\n",
    "        - tokenize by splitting the string on whitespace \n",
    "        - removes any single character tokens\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        message : The text message to be preprocessed.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        tokens: The preprocessed text into tokens.\n",
    "    \"\"\" \n",
    "    \n",
    "    # Lowercase the twit message\n",
    "    text = message.lower()\n",
    "    \n",
    "    # Replace URLs with a space in the message\n",
    "    text = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)', ' ', text)\n",
    "    \n",
    "    # Replace ticker symbols with a space. The ticker symbols are any stock symbol that starts with $.\n",
    "    text = re.sub(r'^\\$[A-Z]+', ' ', text)\n",
    "    \n",
    "    # Replace StockTwits usernames with a space. The usernames are any word that starts with @.\n",
    "    text = re.sub(r'^\\@\\w+', ' ', text)\n",
    "\n",
    "    # Replace everything not a letter with a space\n",
    "    text = re.sub(r'[^A-Za-z]', ' ', text)\n",
    "    \n",
    "    # Tokenize by splitting the string on whitespace into a list of words\n",
    "    tokens = text.split(' ')\n",
    "\n",
    "    # Lemmatize words using the WordNetLemmatizer. You can ignore any word that is not longer than one character.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [wnl.lemmatize(w) for w in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess All the Twits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [preprocess(m) for m in messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\"\"\"\n",
    "Create a vocabulary by using Bag of words\n",
    "\"\"\"\n",
    "    \n",
    "bow = Counter(word for message in tokenized for word in message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of Words Appearing in Message\n",
    "With the vocabulary, now I'll remove some of the most common words such as 'the', 'and', 'it', etc. These words don't contribute to identifying sentiment and are really common, resulting in a lot of noise in our input. If I can filter these out, then the network should have an easier time learning. I also want to remove really rare words that show up in a only a few twits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set the following variables:\n",
    "    freqs\n",
    "    low_cutoff\n",
    "    high_cutoff\n",
    "    K_most_common\n",
    "\"\"\"\n",
    "\n",
    "# Dictionart that contains the Frequency of words appearing in messages.\n",
    "# The key is the token and the value is the frequency of that word in the corpus.\n",
    "total = sum(bow.values())\n",
    "\n",
    "freqs = {k: np.log(v/total) for k, v in bow.items()}\n",
    "\n",
    "freq_list = sorted(list(freqs.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Float that is the frequency cutoff. Drop words with a frequency that is lower or equal to this number.\n",
    "low_cutoff = np.log(7 * 1e-7)\n",
    "\n",
    "# Integer that is the cut off for most common words. Drop words that are the `high_cutoff` most common words.\n",
    "high_cutoff = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x7f5a8c879c88>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5a9b61a4e0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(freq_list, marker='o', alpha=0.1)\n",
    "plt.axhline(low_cutoff, color='r')\n",
    "plt.axvline(len(freq_list) - high_cutoff, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'the', 'to', 'a', 'amp', 'is', 'for', 'utm', 'amd', 'on', 'of', 'and', 'in', 's', 'this', 'it', 'i', 'at', 'mu', 'amzn']\n"
     ]
    }
   ],
   "source": [
    "# The k most common words in the corpus. Use `high_cutoff` as the k.\n",
    "K_most_common = [k for (k, v) in bow.most_common()[:high_cutoff]]\n",
    "print(K_most_common)\n",
    "\n",
    "filtered_words = [word for word in freqs if (freqs[word] > low_cutoff) & (word not in K_most_common)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109530, 20, 13886)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(freq_list), len(K_most_common), len(filtered_words) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Vocabulary by Removing Filtered Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set the following variables:\n",
    "    vocab\n",
    "    id2vocab\n",
    "    filtered\n",
    "\"\"\"\n",
    "\n",
    "# A dictionary for the `filtered_words`. The key is the word and value is an id that represents the word. \n",
    "vocab = {word:i for (i, word) in enumerate(filtered_words)}\n",
    "# Reverse of the `vocab` dictionary. The key is word id and value is the word. \n",
    "id2vocab = {v:k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized with the words not in `filtered_words` removed.\n",
    "filtered_words_set = set(filtered_words)\n",
    "filtered = [list(set(message).intersection(filtered_words_set)) for message in tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the classes\n",
    "50% of the twits are neutral. This means that the network will be 50% accurate just by guessing 0 every single time. To help the network learn appropriately, I'll want to balance our classes. That is, make sure each of the different sentiment scores show up roughly as frequently in the data.\n",
    "\n",
    "What I can do here is go through each of the examples and randomly drop twits with neutral sentiment. I should also take this opportunity to remove messages with length 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced = {'messages': [], 'sentiments':[]}\n",
    "\n",
    "n_neutral = sum(1 for each in sentiments if each == 2)\n",
    "N_examples = len(sentiments)\n",
    "keep_prob = (N_examples - n_neutral)/4/n_neutral\n",
    "\n",
    "for idx, sentiment in enumerate(sentiments):\n",
    "    message = filtered[idx]\n",
    "    if len(message) == 0:\n",
    "        # skip this message because it has length zero\n",
    "        continue\n",
    "    elif sentiment != 2 or random.random() < keep_prob:\n",
    "        balanced['messages'].append(message)\n",
    "        balanced['sentiments'].append(sentiment) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19825485327766276"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neutral = sum(1 for each in balanced['sentiments'] if each == 2)\n",
    "N_examples = len(balanced['sentiments'])\n",
    "n_neutral/N_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tokens into integer ids which can be passed to the network\n",
    "token_ids = [[vocab[word] for word in message] for message in balanced['messages']]\n",
    "sentiments = balanced['sentiments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "Now I have the vocabulary which means I can transform the tokens into ids, which are then passed to our network. I'll define the network.\n",
    "\n",
    "Here is a diagram showing the network I'd like to build: \n",
    "\n",
    "#### Embed -> RNN -> Linear -> Softmax\n",
    "### Implement the text classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size, n_layers=1, drop_prob=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(TextClassifier, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=False)\n",
    "        \n",
    "        # linear layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=1)        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        x = x.long().to(device)\n",
    "        embeds = self.embedding(x)\n",
    "\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out[-1, :, :]\n",
    "        fc_out = self.fc(lstm_out)\n",
    "        soft_out = self.softmax(fc_out)\n",
    "\n",
    "        return soft_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextClassifier(\n",
      "  (embedding): Embedding(13886, 10, padding_idx=0)\n",
      "  (lstm): LSTM(10, 6, num_layers=2, dropout=0.1)\n",
      "  (fc): Linear(in_features=6, out_features=5, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab) \n",
    "output_size = 5\n",
    "embedding_dim = 10\n",
    "hidden_dim = 6\n",
    "n_layers = 2\n",
    "drop_prob = 0.1\n",
    "model = TextClassifier(vocab_size, embedding_dim, hidden_dim, output_size, n_layers, drop_prob)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6107, -1.8438, -1.6509, -1.2093, -1.8855],\n",
      "        [-1.5985, -1.8774, -1.6373, -1.2057, -1.8915],\n",
      "        [-1.5841, -1.8916, -1.6280, -1.2101, -1.9001],\n",
      "        [-1.5685, -1.9265, -1.6303, -1.2007, -1.9031]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "input = torch.randint(0, 1000, (5, 4), dtype=torch.int64)\n",
    "hidden = model.init_hidden(batch_size)\n",
    "\n",
    "logps, _ = model.forward(input, hidden)\n",
    "print(logps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "### DataLoaders and Batching\n",
    "Now I'll build a generator that I can use to loop through our data. It'll be more efficient if I can pass our sequences in as batches. The input tensors should look like `(sequence_length, batch_size)`. So if the sequences are 40 tokens long and I pass in 25 sequences, then I'd have an input size of `(40, 25)`.\n",
    "\n",
    "For messages with fewer than 40 tokens, I will pad the empty spots with zeros. I'll **left** pad so that the RNN starts from nothing before going through the data. If the message has 20 tokens, then the first 20 spots of the 40 long sequence will be 0. If a message has more than 40 tokens, I'll just keep the first 40 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(messages, labels, sequence_length=30, batch_size=32, shuffle=False):\n",
    "    \"\"\" \n",
    "    Build a dataloader.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        indices = list(range(len(messages)))\n",
    "        random.shuffle(indices)\n",
    "        messages = [messages[idx] for idx in indices]\n",
    "        labels = [labels[idx] for idx in indices]\n",
    "\n",
    "    total_sequences = len(messages)\n",
    "\n",
    "    for ii in range(0, total_sequences, batch_size):\n",
    "        batch_messages = messages[ii: ii+batch_size]\n",
    "        \n",
    "        # First initialize a tensor of all zeros\n",
    "        batch = torch.zeros((sequence_length, len(batch_messages)), dtype=torch.int64)\n",
    "        for batch_num, tokens in enumerate(batch_messages):\n",
    "            token_tensor = torch.tensor(tokens)\n",
    "            # Left pad!\n",
    "            start_idx = max(sequence_length - len(token_tensor), 0)\n",
    "            batch[start_idx:, batch_num] = token_tensor[:sequence_length]\n",
    "        \n",
    "        label_tensor = torch.tensor(labels[ii: ii+len(batch_messages)])\n",
    "        \n",
    "        yield batch, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and  Validation\n",
    "With the data in nice shape, I'll split it into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split data into training and validation datasets. Use an appropriate split size.\n",
    "The features are the `token_ids` and the labels are the `sentiments`.\n",
    "\"\"\"   \n",
    "size = int(len(token_ids) * 0.8)\n",
    "train_features = token_ids[:size]\n",
    "valid_features = token_ids[size:]\n",
    "train_labels = sentiments[:size]\n",
    "valid_labels = sentiments[size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch, labels = next(iter(dataloader(train_features, train_labels, sequence_length=20, batch_size=64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextClassifier(\n",
      "  (embedding): Embedding(13887, 200, padding_idx=0)\n",
      "  (lstm): LSTM(200, 128, num_layers=2, dropout=0.1)\n",
      "  (fc): Linear(in_features=128, out_features=5, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab) + 1\n",
    "output_size = 5\n",
    "embedding_dim = 200\n",
    "hidden_dim = 128\n",
    "n_layers = 2\n",
    "drop_prob = 0.1\n",
    "model = TextClassifier(vocab_size, embedding_dim, hidden_dim, output_size, n_layers, drop_prob)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = model.init_hidden(labels.shape[0])\n",
    "logps, hidden = model.forward(text_batch, hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextClassifier(\n",
      "  (embedding): Embedding(13887, 1024, padding_idx=0)\n",
      "  (lstm): LSTM(1024, 256, num_layers=2, dropout=0.2)\n",
      "  (fc): Linear(in_features=256, out_features=5, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab) + 1\n",
    "output_size = 5\n",
    "embedding_dim = 1024\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "drop_prob = 0.2\n",
    "model = TextClassifier(vocab_size, embedding_dim, hidden_dim, output_size, n_layers, drop_prob)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5... Step: 100... Loss: 0.999000... Val Loss: 1.059158 Accuracy: 0.576685\n",
      "Epoch: 1/5... Step: 200... Loss: 0.972933... Val Loss: 0.963300 Accuracy: 0.619526\n",
      "Epoch: 1/5... Step: 300... Loss: 0.841275... Val Loss: 0.929947 Accuracy: 0.631839\n",
      "Epoch: 1/5... Step: 400... Loss: 0.899137... Val Loss: 0.912087 Accuracy: 0.640206\n",
      "Epoch: 1/5... Step: 500... Loss: 0.867356... Val Loss: 0.884901 Accuracy: 0.652811\n",
      "Epoch: 1/5... Step: 600... Loss: 0.837871... Val Loss: 0.878843 Accuracy: 0.652272\n",
      "Epoch: 1/5... Step: 700... Loss: 0.886134... Val Loss: 0.864575 Accuracy: 0.658429\n",
      "Epoch: 1/5... Step: 800... Loss: 0.815470... Val Loss: 0.853947 Accuracy: 0.665808\n",
      "Epoch: 1/5... Step: 900... Loss: 0.823644... Val Loss: 0.852230 Accuracy: 0.666027\n",
      "Epoch: 1/5... Step: 1000... Loss: 0.792486... Val Loss: 0.845881 Accuracy: 0.665913\n",
      "Epoch: 1/5... Step: 1100... Loss: 0.811531... Val Loss: 0.845677 Accuracy: 0.665206\n",
      "Epoch: 1/5... Step: 1200... Loss: 0.769030... Val Loss: 0.836875 Accuracy: 0.669821\n",
      "Epoch: 1/5... Step: 1300... Loss: 0.755583... Val Loss: 0.831503 Accuracy: 0.672351\n",
      "Epoch: 1/5... Step: 1400... Loss: 0.817568... Val Loss: 0.827984 Accuracy: 0.674876\n",
      "Epoch: 1/5... Step: 1500... Loss: 0.782702... Val Loss: 0.822375 Accuracy: 0.676394\n",
      "Epoch: 1/5... Step: 1600... Loss: 0.733982... Val Loss: 0.822496 Accuracy: 0.676203\n",
      "Epoch: 2/5... Step: 1700... Loss: 0.705485... Val Loss: 0.824382 Accuracy: 0.675964\n",
      "Epoch: 2/5... Step: 1800... Loss: 0.683564... Val Loss: 0.823890 Accuracy: 0.676394\n",
      "Epoch: 2/5... Step: 1900... Loss: 0.782596... Val Loss: 0.824597 Accuracy: 0.674642\n",
      "Epoch: 2/5... Step: 2000... Loss: 0.749589... Val Loss: 0.820805 Accuracy: 0.675611\n",
      "Epoch: 2/5... Step: 2100... Loss: 0.762163... Val Loss: 0.819559 Accuracy: 0.677081\n",
      "Epoch: 2/5... Step: 2200... Loss: 0.799573... Val Loss: 0.817782 Accuracy: 0.678852\n",
      "Epoch: 2/5... Step: 2300... Loss: 0.833628... Val Loss: 0.815710 Accuracy: 0.679615\n",
      "Epoch: 2/5... Step: 2400... Loss: 0.643048... Val Loss: 0.812282 Accuracy: 0.677773\n",
      "Epoch: 2/5... Step: 2500... Loss: 0.764223... Val Loss: 0.814946 Accuracy: 0.676933\n",
      "Epoch: 2/5... Step: 2600... Loss: 0.744981... Val Loss: 0.817038 Accuracy: 0.674752\n",
      "Epoch: 2/5... Step: 2700... Loss: 0.745351... Val Loss: 0.812711 Accuracy: 0.678475\n",
      "Epoch: 2/5... Step: 2800... Loss: 0.732501... Val Loss: 0.808761 Accuracy: 0.681467\n",
      "Epoch: 2/5... Step: 2900... Loss: 0.735479... Val Loss: 0.811985 Accuracy: 0.678771\n",
      "Epoch: 2/5... Step: 3000... Loss: 0.762490... Val Loss: 0.808902 Accuracy: 0.681835\n",
      "Epoch: 2/5... Step: 3100... Loss: 0.753107... Val Loss: 0.803202 Accuracy: 0.683071\n",
      "Epoch: 2/5... Step: 3200... Loss: 0.793216... Val Loss: 0.813681 Accuracy: 0.680226\n",
      "Epoch: 3/5... Step: 3300... Loss: 0.695580... Val Loss: 0.815444 Accuracy: 0.680780\n",
      "Epoch: 3/5... Step: 3400... Loss: 0.689591... Val Loss: 0.822047 Accuracy: 0.680451\n",
      "Epoch: 3/5... Step: 3500... Loss: 0.727240... Val Loss: 0.824965 Accuracy: 0.678298\n",
      "Epoch: 3/5... Step: 3600... Loss: 0.657446... Val Loss: 0.824559 Accuracy: 0.679830\n",
      "Epoch: 3/5... Step: 3700... Loss: 0.663738... Val Loss: 0.825407 Accuracy: 0.678069\n",
      "Epoch: 3/5... Step: 3800... Loss: 0.690706... Val Loss: 0.824607 Accuracy: 0.677472\n",
      "Epoch: 3/5... Step: 3900... Loss: 0.681813... Val Loss: 0.818048 Accuracy: 0.680661\n",
      "Epoch: 3/5... Step: 4000... Loss: 0.694697... Val Loss: 0.820553 Accuracy: 0.679568\n",
      "Epoch: 3/5... Step: 4100... Loss: 0.707962... Val Loss: 0.825162 Accuracy: 0.678498\n",
      "Epoch: 3/5... Step: 4200... Loss: 0.752510... Val Loss: 0.816214 Accuracy: 0.681348\n",
      "Epoch: 3/5... Step: 4300... Loss: 0.690157... Val Loss: 0.816728 Accuracy: 0.681152\n",
      "Epoch: 3/5... Step: 4400... Loss: 0.688342... Val Loss: 0.821265 Accuracy: 0.682384\n",
      "Epoch: 3/5... Step: 4500... Loss: 0.764051... Val Loss: 0.814292 Accuracy: 0.681429\n",
      "Epoch: 3/5... Step: 4600... Loss: 0.699203... Val Loss: 0.813248 Accuracy: 0.682121\n",
      "Epoch: 3/5... Step: 4700... Loss: 0.693559... Val Loss: 0.815308 Accuracy: 0.681548\n",
      "Epoch: 3/5... Step: 4800... Loss: 0.739168... Val Loss: 0.816300 Accuracy: 0.679491\n",
      "Epoch: 3/5... Step: 4900... Loss: 0.732159... Val Loss: 0.811651 Accuracy: 0.680126\n",
      "Epoch: 4/5... Step: 5000... Loss: 0.586806... Val Loss: 0.850234 Accuracy: 0.679653\n",
      "Epoch: 4/5... Step: 5100... Loss: 0.582615... Val Loss: 0.856396 Accuracy: 0.674489\n",
      "Epoch: 4/5... Step: 5200... Loss: 0.636005... Val Loss: 0.853706 Accuracy: 0.675458\n",
      "Epoch: 4/5... Step: 5300... Loss: 0.673531... Val Loss: 0.860930 Accuracy: 0.672241\n",
      "Epoch: 4/5... Step: 5400... Loss: 0.603008... Val Loss: 0.856207 Accuracy: 0.674212\n",
      "Epoch: 4/5... Step: 5500... Loss: 0.638809... Val Loss: 0.864873 Accuracy: 0.674776\n",
      "Epoch: 4/5... Step: 5600... Loss: 0.697376... Val Loss: 0.852035 Accuracy: 0.675592\n",
      "Epoch: 4/5... Step: 5700... Loss: 0.676372... Val Loss: 0.850986 Accuracy: 0.678021\n",
      "Epoch: 4/5... Step: 5800... Loss: 0.624576... Val Loss: 0.851854 Accuracy: 0.674718\n",
      "Epoch: 4/5... Step: 5900... Loss: 0.608417... Val Loss: 0.852221 Accuracy: 0.675320\n",
      "Epoch: 4/5... Step: 6000... Loss: 0.585315... Val Loss: 0.852251 Accuracy: 0.676284\n",
      "Epoch: 4/5... Step: 6100... Loss: 0.679724... Val Loss: 0.851778 Accuracy: 0.674742\n",
      "Epoch: 4/5... Step: 6200... Loss: 0.688009... Val Loss: 0.841494 Accuracy: 0.677878\n",
      "Epoch: 4/5... Step: 6300... Loss: 0.644522... Val Loss: 0.854325 Accuracy: 0.678069\n",
      "Epoch: 4/5... Step: 6400... Loss: 0.644114... Val Loss: 0.838850 Accuracy: 0.678274\n",
      "Epoch: 4/5... Step: 6500... Loss: 0.615468... Val Loss: 0.843455 Accuracy: 0.673998\n",
      "Epoch: 5/5... Step: 6600... Loss: 0.520695... Val Loss: 0.895625 Accuracy: 0.673301\n",
      "Epoch: 5/5... Step: 6700... Loss: 0.554158... Val Loss: 0.911385 Accuracy: 0.670924\n",
      "Epoch: 5/5... Step: 6800... Loss: 0.569391... Val Loss: 0.915563 Accuracy: 0.671272\n",
      "Epoch: 5/5... Step: 6900... Loss: 0.533203... Val Loss: 0.918404 Accuracy: 0.666204\n",
      "Epoch: 5/5... Step: 7000... Loss: 0.607547... Val Loss: 0.923139 Accuracy: 0.665970\n",
      "Epoch: 5/5... Step: 7100... Loss: 0.521609... Val Loss: 0.918868 Accuracy: 0.670261\n",
      "Epoch: 5/5... Step: 7200... Loss: 0.583870... Val Loss: 0.906817 Accuracy: 0.669755\n",
      "Epoch: 5/5... Step: 7300... Loss: 0.549603... Val Loss: 0.909778 Accuracy: 0.669378\n",
      "Epoch: 5/5... Step: 7400... Loss: 0.534628... Val Loss: 0.900795 Accuracy: 0.670919\n",
      "Epoch: 5/5... Step: 7500... Loss: 0.567540... Val Loss: 0.902923 Accuracy: 0.669010\n",
      "Epoch: 5/5... Step: 7600... Loss: 0.538207... Val Loss: 0.910016 Accuracy: 0.668127\n",
      "Epoch: 5/5... Step: 7700... Loss: 0.582220... Val Loss: 0.906640 Accuracy: 0.671430\n",
      "Epoch: 5/5... Step: 7800... Loss: 0.573368... Val Loss: 0.901875 Accuracy: 0.667497\n",
      "Epoch: 5/5... Step: 7900... Loss: 0.593491... Val Loss: 0.900809 Accuracy: 0.668991\n",
      "Epoch: 5/5... Step: 8000... Loss: 0.563718... Val Loss: 0.894399 Accuracy: 0.670103\n",
      "Epoch: 5/5... Step: 8100... Loss: 0.575579... Val Loss: 0.899270 Accuracy: 0.673162\n"
     ]
    }
   ],
   "source": [
    "# training params\n",
    "epochs = 5\n",
    "batch_size = 512\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "model.train()\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in dataloader(\n",
    "            train_features, train_labels, batch_size=batch_size, sequence_length=20, shuffle=True):\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # initialize hidden state\n",
    "        h = model.init_hidden(labels.shape[0])\n",
    "        for each in h:\n",
    "            each.to(device)        \n",
    "\n",
    "        correct = 0\n",
    "        total = 1\n",
    "        \n",
    "        # zero accumulated gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # get the output from the model\n",
    "        output, h = model(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            \n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inputs, labels in dataloader(\n",
    "            valid_features, valid_labels, batch_size=batch_size, sequence_length=20, shuffle=True):\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                    \n",
    "                val_h = model.init_hidden(labels.shape[0])\n",
    "                for each in val_h:\n",
    "                    each.to(device) \n",
    "\n",
    "                \n",
    "                output, val_h = model(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels)\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "            model.train()\n",
    "            \n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)),\n",
    "                  \"Accuracy: {:.6f}\".format(correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = TextClassifier(vocab_size, embedding_dim, hidden_dim, output_size, n_layers, drop_prob)\n",
    "trained_model.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "### Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model, vocab):\n",
    "    \"\"\" \n",
    "    Make a prediction on a single sentence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        text : The string to make a prediction on.\n",
    "        model : The model to use for making the prediction.\n",
    "        vocab : Dictionary for word to word ids. The key is the word and the value is the word id.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        pred : Prediction vector\n",
    "    \"\"\"    \n",
    "    \n",
    "    tokens = preprocess(text)\n",
    "    \n",
    "    # Filter non-vocab words\n",
    "    tokens = [word for word in tokens if word in vocab]\n",
    "\n",
    "    # Convert words to ids\n",
    "    tokens = [vocab[word] for word in tokens]\n",
    "\n",
    "    # Adding a batch dimension\n",
    "    text_array = torch.tensor(tokens).view(-1,1)\n",
    "\n",
    "    # Get the NN output\n",
    "    hidden = model.init_hidden(1)\n",
    "\n",
    "    logps, _ = model.forward(text_array, hidden)\n",
    "    \n",
    "    # Take the exponent of the NN output to get a range of 0 to 1 for each label.\n",
    "    prediction = torch.exp(logps)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0004,  0.0172,  0.0203,  0.5361,  0.4261]], device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Nashville is going to reopen, which signals increased economic activities\"\n",
    "model.eval()\n",
    "model.to(device)\n",
    "predict(text, model, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the prediction of the model? What is the uncertainty of the prediction?\n",
    "The model prediction is a probability distribution of the five classes of the sentiment of the tweet. The class with the highest probability score is the most likely sentiment level based on the model. \n",
    "\n",
    "The model is predicting a probability (54%) of the text being positive and being a sentiment of 1. If combining the probabilities on the positive side (54%+43%), the model predicts a positive sentiment overall. Which I agree with intuitively based on the text. The uncertainly of this prediction being 1 is 47%. The uncertainly of this prediction being positive is 3% which is quite low. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Now I have a trained model and I can make predictions. I can use this model to track the sentiments of various stocks by predicting the sentiments of twits as they are coming in. Now I have a stream of twits. For each of those twits, I'll pull out the stocks mentioned in them and keep track of the sentiments. I want to track the sentiments of the stocks in the universe and use this as a signal in the larger model(s).\n",
    "\n",
    "### Load the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('..', '..', 'data', 'project_6_stocktwits', 'test_twits.json'), 'r') as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twit Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message_body': '$JWN has moved -1.69% on 10-31. Check out the movement and peers at  https://dividendbot.com?s=JWN',\n",
       " 'timestamp': '2018-11-01T00:00:05Z'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def twit_stream():\n",
    "    for twit in test_data['data']:\n",
    "        yield twit\n",
    "\n",
    "next(twit_stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `prediction` function, I'll apply it to a stream of twits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_twits(stream, model, vocab, universe):\n",
    "    \"\"\" \n",
    "    Given a stream of twits and a universe of tickers, return sentiment scores for tickers in the universe.\n",
    "    \"\"\"\n",
    "    for twit in stream:\n",
    "\n",
    "        # Get the message text\n",
    "        text = twit['message_body']\n",
    "        symbols = re.findall('\\$[A-Z]{2,4}', text)\n",
    "        score = predict(text, model, vocab)\n",
    "\n",
    "        for symbol in symbols:\n",
    "            if symbol in universe:\n",
    "                yield {'symbol': symbol, 'score': score, 'timestamp': twit['timestamp']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'symbol': '$AAPL',\n",
       " 'score': tensor([[ 0.1307,  0.1637,  0.2383,  0.0851,  0.3823]], device='cuda:0'),\n",
       " 'timestamp': '2018-11-01T00:00:18Z'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "universe = {'$BBRY', '$AAPL', '$AMZN', '$BABA', '$YHOO', '$LQMT', '$FB', '$GOOG', '$BBBY', '$JNUG', '$SBUX', '$MU'}\n",
    "score_stream = score_twits(twit_stream(), model, vocab, universe)\n",
    "\n",
    "next(score_stream)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
